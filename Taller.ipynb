{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import tensorflow as tf\n",
    "## https://mmuratarat.github.io/2019-01-07/logistic-regression-in-Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_example(example, label, dim=(28, 28)):\n",
    "    \"\"\"\n",
    "    Displays a single example and label.\n",
    "    If example is already unrolled from (28, 28) to (784,), it will be reshaped first.\n",
    "    \"\"\"\n",
    "    example = np.squeeze(example)\n",
    "    label = np.squeeze(label)\n",
    "    if example.shape != dim:\n",
    "        example = example.reshape(dim)\n",
    "    plt.imshow(example, cmap='binary')\n",
    "    plt.xlabel('Label: ' + str(int(label)))\n",
    "    plt.yticks([])\n",
    "    plt.xticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_example(x_test[0], y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x train:  (60000, 28, 28)\n",
      "Shape of y train:  (60000,)\n",
      "Shape of x test:  (10000, 28, 28)\n",
      "Shape of y test:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of x train: ', x_train.shape)\n",
    "print('Shape of y train: ', y_train.shape)\n",
    "print('Shape of x test: ', x_test.shape)\n",
    "print('Shape of y test: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset parameters.\n",
    "num_classes = 10 # 0 to 9 digits\n",
    "num_features = 784 # 28*28\n",
    "\n",
    "# Training parameters.\n",
    "learning_rate = 0.01\n",
    "training_steps = 1000\n",
    "batch_size = 256\n",
    "display_step = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# Convert to float32.\n",
    "x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n",
    "# Flatten images to 1-D vector of 784 features (28*28).\n",
    "x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])\n",
    "# Normalize images value from [0, 255] to [0, 1].\n",
    "x_train, x_test = x_train / 255., x_test / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use tf.data API to shuffle and batch data.\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_data = train_data.repeat().shuffle(5000).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight of shape [784, 10], the 28*28 image features, and total number of classes.\n",
    "W = tf.Variable(tf.ones([num_features, num_classes]), name=\"weight\")\n",
    "# Bias of shape [10], the total number of classes.\n",
    "b = tf.Variable(tf.zeros([num_classes]), name=\"bias\")\n",
    "\n",
    "# Logistic regression (Wx + b).\n",
    "def logistic_regression(x):\n",
    "    # Apply softmax to normalize the logits to a probability distribution.\n",
    "    return tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "# Cross-Entropy loss function.\n",
    "def cross_entropy(y_pred, y_true):\n",
    "    # Encode label to a one hot vector.\n",
    "    y_true = tf.one_hot(y_true, depth=num_classes)\n",
    "    # Clip prediction values to avoid log(0) error.\n",
    "    y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)\n",
    "    # Compute cross-entropy.\n",
    "    return tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred) + (1-y_true) * tf.math.log(1-y_pred),1))\n",
    "\n",
    "# Accuracy metric.\n",
    "def accuracy(y_pred, y_true):\n",
    "    # Predicted class is the index of highest score in prediction vector (i.e. argmax).\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Stochastic gradient descent optimizer.\n",
    "optimizer = tf.optimizers.SGD(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization process. \n",
    "def run_optimization(x, y):\n",
    "    # Wrap computation inside a GradientTape for automatic differentiation.\n",
    "    with tf.GradientTape() as g:\n",
    "        pred = logistic_regression(x)\n",
    "        loss = cross_entropy(pred, y)\n",
    "\n",
    "    # Compute gradients.\n",
    "    gradients = g.gradient(loss, [W, b])\n",
    "    \n",
    "    # Update W and b following gradients.\n",
    "    optimizer.apply_gradients(zip(gradients, [W, b]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 50, loss: 2.645669, accuracy: 0.710938\n",
      "step: 100, loss: 2.241494, accuracy: 0.773438\n",
      "step: 150, loss: 2.011868, accuracy: 0.757812\n",
      "step: 200, loss: 1.661606, accuracy: 0.800781\n",
      "step: 250, loss: 1.545889, accuracy: 0.835938\n",
      "step: 300, loss: 1.426264, accuracy: 0.816406\n",
      "step: 350, loss: 1.384900, accuracy: 0.835938\n",
      "step: 400, loss: 1.190131, accuracy: 0.859375\n",
      "step: 450, loss: 1.098926, accuracy: 0.890625\n",
      "step: 500, loss: 1.263881, accuracy: 0.824219\n",
      "step: 550, loss: 1.140792, accuracy: 0.832031\n",
      "step: 600, loss: 1.109887, accuracy: 0.859375\n",
      "step: 650, loss: 1.025234, accuracy: 0.835938\n",
      "step: 700, loss: 0.984845, accuracy: 0.867188\n",
      "step: 750, loss: 1.032359, accuracy: 0.847656\n",
      "step: 800, loss: 0.825284, accuracy: 0.898438\n",
      "step: 850, loss: 0.940616, accuracy: 0.855469\n",
      "step: 900, loss: 0.923972, accuracy: 0.882812\n",
      "step: 950, loss: 0.893590, accuracy: 0.867188\n",
      "step: 1000, loss: 0.814020, accuracy: 0.878906\n"
     ]
    }
   ],
   "source": [
    "# Run training for the given number of steps.\n",
    "for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):\n",
    "    # Run the optimization to update W and b values.\n",
    "    run_optimization(batch_x, batch_y)\n",
    "    \n",
    "    if step % display_step == 0:\n",
    "        pred = logistic_regression(batch_x)\n",
    "        loss = cross_entropy(pred, batch_y)\n",
    "        acc = accuracy(pred, batch_y)\n",
    "        print(\"step: %i, loss: %f, accuracy: %f\" % (step, loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.875800\n"
     ]
    }
   ],
   "source": [
    "# Test model on validation set.\n",
    "pred = logistic_regression(x_test)\n",
    "print(\"Test Accuracy: %f\" % accuracy(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "closing parenthesis ']' does not match opening parenthesis '(' (<ipython-input-39-7fc44a7f41b8>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-39-7fc44a7f41b8>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    print(\"Model prediction: {}, probability: {} \".format(np.argmax(predictions.numpy()[i]), predictions.numpy()[i][np.argmax(predictions.numpy()[i]]))\u001b[0m\n\u001b[0m                                                                                                                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m closing parenthesis ']' does not match opening parenthesis '('\n"
     ]
    }
   ],
   "source": [
    "# Predict 5 images from validation set.\n",
    "n_images = 5\n",
    "test_images = x_test[:n_images]\n",
    "predictions = logistic_regression(test_images)\n",
    "\n",
    "# Display image and model prediction.\n",
    "for i in range(n_images):\n",
    "    show_example(test_images[i], np.argmax(predictions.numpy()[i]))\n",
    "    plt.show()\n",
    "    print(\"Model prediction: {}, probability: {} \".format(np.argmax(predictions.numpy()[i]), predictions.numpy()[i][np.argmax(predictions.numpy()[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze(x_train[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[[0], [1], [2]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfg = tf.data.Dataset.from_tensor_slices(([[5, 10], [3, 6]],[1, 2]) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = x_test[:n_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.7979834e-02, 6.0696522e-04, 4.7364768e-02, 5.9391703e-03,\n",
       "       6.8933511e-01, 1.1244542e-02, 3.3042233e-02, 5.0944634e-02,\n",
       "       3.4082264e-02, 1.0946045e-01], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.numpy()[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
