{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Álgebra lineal y redes neuronales\n",
    "Las redes neuronales son modelos cuantitativos que aprende como asociar una «entrada» y «patrón adaptado de salida» con el uso de algoritmos de aprendizaje. El objetivo aquí, será exponer cuatro conceptos principales del álgebra lineal que son esenciales para el análisis de estos modelos: 1) la proyección de un vector, 2) la descomposición por valores propios y singulares, 3) el gradiente de una matriz Hessiana de una función vectorial, y 4) la expansión en Taylor de una función vectorial. Estos conceptos son ilustrados con el análisis de las reglas de Hebbian y Widrow-Hoff y algunas arquitecturas simples de las redes neuronales (es decir, el auto asociador lineal, el hetero asociador y el error de redes por propagación regresiva). Se muestra también que las redes neuronales son equivalente a versiones iterativas de la estadística estándar y modelos de optimización tales como el análisis de regresión multiple y el análisis por componentes principales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El álgebra lineal es usada particularmente para analizar la clase de redes neuronales denominadas «asociadores». Estos modelos de aprendizaje cuantitativo asocian una «entrada» y una «salida» mediante patrones adaptativos con el uso de algoritmos de aprendizaje. Cuando el conjunto de patrones de entrada es diferente del conjunto del patrón de salida, los modelos se denominan <em>heteroasociadores</em>. Cuando los patrones de entrada y los de salida son iguales, el modelo se denomina <em>autoasociador</em>. Los asociadores consisten de capas de unidades elementales denominadas <em>neuronas</em>. La información fluye a través de todas las capas. Algunas arquitecturas puede incluir capas intermedias (capas ocultas). Típicamente las neuronas de una capa están conectadas con las neuronas de otra capa. Las operaciones del álgebra lineal describe las transformaciones de la información a través de cada una de las capas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como es usual, los vectores serán representados por letras minúsculas (e.g., $x$), las matrices por letras mayúsculas (e.g., $X$). Además se supone que las siguientes nociones son conocidas: La operación de transposición (e.g., $x^\\top$), la norma de un vector (e.g.,$||x||$), el producto escalar (e.g., $x^{\\top}w$) y el producto de dos matrices (e.g., $AB)$. También se usará el producto de Hadamard (e.g., $A\\otimes B$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proyección"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coseno entre dos vectores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El <em>coseno</em> de dos vectores $x$ y $y$ es el coseno del ángulo formado por el origen del espacio y los puntos definidos por las coordenadas de los vectores. Por lo tanto,\n",
    "\n",
    "\\begin{equation}\\nonumber\n",
    "\\cos(x, y) = \\frac{x^\\top y}{||x||||y||}.\n",
    "\\end{equation}\n",
    "\n",
    "El coseno indica la <em>similaridad</em> entre los vectores. Cuando dos vectores son proporcionales, es decir tienen la misma dirección, su coseno es igual a uno; si tienen dirección opuesta, su coseno es igual a menos uno; y cuando ellos son ortogonales, su coseno es igual a cero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distancia entre vectores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entre una gran familia de distancias entre vectores, la más popular es la distancia euclidiana. Ésta está relacionada con el coseno entre vectores y se define como\n",
    "\n",
    "\\begin{equation}\\nonumber\n",
    "d_{_2}(x,y)=\\sqrt{(x-y)^\\top(x-y)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proyección de un vector sobre otro vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " La proyección ortogonal de un vector $x$ sobre un vector $w$ se define como:\n",
    " \n",
    "\\begin{equation}\\nonumber\n",
    "\\operatorname{proy}_{\\langle w\\rangle} x = \\frac{x^{\\top} w}{w^{\\top}w}w.\n",
    "\\end{equation}\n",
    "\n",
    "La norma de $\\operatorname*{proy}_{\\langle w\\rangle} x$ es su distancia al origen del espacio. Esto es igual a:\n",
    "\n",
    "\\begin{equation}\\nonumber\n",
    "||\\operatorname{proy}_{\\langle w\\rangle} x|| = \\frac{|x^{\\top}w|}{||w||}= |\\cos(x,y)|||x||.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Las reglas de aprendizaje de Hebbian y Widrow-Hoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una red neuronal consiste de células conectadas a otras células vía conexiones de peso denominadas sinapsis. Considere una red neuronal de $I$ entradas dada por una capa de células y solo una célula de salida. La información es transmitida vía la sinapsis, del conjunto de entrada de las células externas a las células de salida con la respuesta correspondiente al estado de activación. Si el patrón de entrada y el conjunto de pesos sinápticos son dados por un vector $I$ - dimensional denotado por $x$, y $w$, la activación de la célula de salida es dada por\n",
    "\n",
    "\\begin{equation}\\nonumber\n",
    "a = x^{\\top}w.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así la activación es proporcional a la norma de la proyección del vector de entradas sobre el vector de pesos. La respuesta o salida de la célula es denotada por $r$. Para una célula lineal, esta es proporcional a la activación (por conveniencia, se asume que la constante de proporcionalidad es igual a uno). Los heteroasociadores lineales y los autoasociadores son construidos con células lineales. En general, la salida de una célula es una función (no necesariamente continua), denominada la función de transferencia, y su activación es:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\label{eqn:función}\n",
    "r = f(a).\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, en redes de retropropagación, la función de transferencia (no lineal) suele ser la función logística\n",
    "\n",
    "\\begin{equation}\\nonumber\n",
    "r = f(a) = \\operatorname{logit}(w^{\\top}x) = \\frac{1}{1 + \\exp\\{-w^{\\top}x\\}}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A menudo,una red neuronal está diseñada para asociar, a una entrada dada, una respuesta específica llamada objetivo, denotada como $t$. El aprendizaje es equivalente a definir una regla que especifique cómo agregar una pequeña cantidad a cada peso sináptico en cada iteración de aprendizaje. El aprendizaje acerca la salida de la red al objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las reglas de aprendizaje vienen en dos sabores principales: <em>supervisadas</em> (por ejemplo, Widrow-Hoff) que tienen en cuenta el error o la distancia entre la respuesta de la neurona y el objetivo,  y <em>sin supervisión</em> (por ejemplo, Hebbian) que no requieren tal «retroalimentación». La regla de aprendizaje hebbiana modifica el vector de peso en la iteración $n + 1$ como\n",
    "\n",
    "\\begin{equation}\\nonumber\n",
    "w_{n+1} = w_{n} + \\eta t x,\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "donde $\\eta$ es una pequeña constante positiva llamada <em>constante de aprendizaje</em>.Entonces, una iteración de aprendizaje hebbiana mueve el vector de peso en la dirección del vector de entrada en una cantidad proporcional al objetivo.\n",
    "\n",
    "La regla de aprendizaje de Widrow-Hoff utiliza el error y la derivada de la función de transferencia $f$ para calcular la corrección como:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eqn:corrección}\n",
    "w_{n+1} = w_{n} + \\eta f'(a)(t-r) x.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces, una iteración de aprendizaje de Widrow-Hoff mueve el vector de peso en la dirección del vector de entrada en una cantidad proporcional al error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para redes con varias celdas (por ejemplo $J$) en la capa de salida, el patrón de activación, salida y objetivo se convierten en vectores $J$ - dimensionales (denotados $a$, $r$ y $t$, respectivamente), y los pesos sinápticos se almacenan en una matriz $W$ de dimensión  $I \\times J$. Las ecuaciones de aprendizaje se convierten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "W_{n+1} = W_{n} + \\eta x t^{\\top} \\hbox{ (Hebbian)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "W_{n+1} = W_{n} + \\eta (f'(a) \\otimes x)(t- r)^{\\top} \\hbox{ (Widrow-Hoff)},\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "en donde la derivada $f'$ aplica sobre $a$ por cada componente, es decir, $f'(a)=(f'(a_{1}),...,f'(a_{J}))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general, se deben aprender varias ($K$) asociaciones de entrada / destino. Luego, el conjunto de patrones de entrada se almacena en una matriz $I \\times K$ denotada como $X$, los patrones de activación y objetivo respectivamente se almacenan en matrices de dimensión $J \\times K$ indicadas como $A$ y $T$, respectivamente. Las iteraciones de activación y aprendizaje se pueden calcular para todos los patrones a la vez (esto se llama modo por lotes). La matriz de salida se calcula como:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "r = f(A) = f(WX^{T}),    \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "en donde   $f$ también aplica sobre cada componente de $WX^{\\top}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las ecuaciones de aprendizaje se convierten\n",
    "\n",
    "\\begin{equation}\n",
    "W_{n+1} = W_{n} + \\eta X T^{\\top} \\hbox{ (Hebbian)},\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "W_{n+1} = W_{n} + \\eta (f'(A) \\otimes X)(T- R)^{\\top} \\hbox{ (Widrow-Hoff).}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valores propios, vectores propios y la descomposición en valores singulares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los vectores propios de una matriz cuadrada $W$ dada (resultante de su <em>descomposición propia</em>) son vectores invariantes bajo multiplicación por $W$. La descomposición propia se define mejor para una subclase de matrices llamadas matrices <em>semi-definidas</em> positivas. Una matriz $X$ es positiva semi-definida si existe otra matriz $Y$ tal que $X = YY^{\\top}$. Este es el caso de la mayoría de las matrices utilizadas en redes neuronales, por lo que se considera solo este caso aquí."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formalmente, un vector (distinto de cero) $u$ es un vector propio de una matriz cuadrada $W$ si\n",
    "\n",
    "\\begin{equation}\n",
    "\\lambda u = Wu.\n",
    "\\end{equation}\n",
    "\n",
    "El escalar $\\lambda$ es el valor propio asociado con $u$. Entonces $u$ es un vector propio de $W$ si su dirección es invariante bajo la multiplicación por $W$ (solo su longitud cambia si $\\lambda \\neq 1$). En general, hay varios vectores propios para una matriz dada (como máximo, la dimensión de $W$).  En general, se ordenan por orden decreciente de su valor propio. Entonces, el primer vector propio, $u_{1}$ tiene el mayor valor propio $\\lambda_{1}$. El número de vectores propios con un valor propio distinto de cero es el rango de la matriz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los valores propios de las matrices semidefinidas positivas son siempre positivos o cero (una matriz con valores propios estrictamente positivos, es definida positiva). Además, cualquiera de los dos vectores propios con valores propios diferentes son ortogonales, es decir:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\\nonumber\n",
    "u_l^{\\top} u_{l'} = 0 \\,\\, \\forall\\,\\, l \\neq l'.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, el conjunto de vectores propios de una matriz constituye una base ortogonal para sus filas y columnas. Esto se expresa definiendo dos matrices: la matriz de vectores propios $U$, y la matriz diagonal de los valores propios: $\\Lambda$. La descomposición propia de $W$ (con rango L) es:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\\nonumber\n",
    "W = U \\Lambda U^{\\top} = \\sum_{l}^{L} \\Lambda_{l} u_{l} u_{l}^{\\top}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La descomposición de valores singulares (SVD) generaliza la descomposición propia en matrices rectangulares. Si $X$ es una matriz $I \\times K$, su SVD se define como:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\\nonumber\n",
    "X = U \\Delta V^{\\top}\n",
    "\\end{equation}\n",
    "\n",
    "con $U U^{\\top} = V^{\\top} V = I$ y $\\Delta$ una matriz diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "($I$ siendo la matriz identidad). Los elementos diagonales de $\\Delta$ son números reales positivos llamados valores singulares de $X$. Las matrices $U$ y $V$ son las matrices izquierda y derecha de vectores singulares (que también son vectores propios, ver más abajo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El SVD está estrechamente relacionado con la descomposición propia porque $U$, $V$ y $\\Delta$ pueden obtenerse a partir de la descomposición propia de las matrices $X^{\\top} X$ y $X X^{\\top}$ como\n",
    "\n",
    "\\begin{equation}\\nonumber\n",
    "X^{\\top} X = U \\Lambda U^{\\top},\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\\nonumber\n",
    "X X^{\\top} = V \\Lambda V^{\\top},\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\\nonumber\n",
    "\\Delta = \\Lambda^{1/2}\n",
    "\\end{equation}\n",
    "\n",
    "(tenga en cuenta que $X^{\\top} X$ y $X X^{\\top}$ tienen los mismos valores propios)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las descomposiciones de valores propios y singulares se utilizan en la mayoría de los campos de las matemáticas aplicadas, incluidas las estadísticas, el procesamiento de imágenes, la mecánica y los sistemas dinámicos. Para las redes neuronales, son esenciales para estudiar la dinámica de los autoasociadores y heteroasociadores lineales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesos iterativos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un heteroasociador lineal que usa la regla de Widrow-Hoff, el aprendizaje modifica solo los valores propios de la matriz de peso. Específicamente, si los patrones a aprender se almacenan en una matriz $I \\times K$ $X$, $X$, con una descomposición de valores singulares como $X = U \\Delta V^{\\top}$, entonces la ecuación de aprendizaje (ver Ecuación \\ref{eqn:Hebbian}) se convierte en, porque para un heteroasociador lineal, $O = A$, y $f'(A) = I$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
