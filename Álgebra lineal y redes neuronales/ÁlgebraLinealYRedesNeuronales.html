<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="&#193;lgebra-lineal-y-redes-neuronales">&#193;lgebra lineal y redes neuronales<a class="anchor-link" href="#&#193;lgebra-lineal-y-redes-neuronales">&#182;</a></h2><p>Las redes neuronales son modelos cuantitativos que aprende como asociar una «entrada» y una «salida» con el uso de algoritmos de aprendizaje. El objetivo aquí, será exponer cuatro conceptos principales del álgebra lineal que son esenciales para el análisis de estos modelos: 1) la proyección de un vector, 2) la descomposición por valores propios y singulares, 3) el gradiente de una matriz Hessiana de una función vectorial, y 4) la expansión en Taylor de una función vectorial. Estos conceptos son ilustrados con el análisis de las reglas de Hebbian y Widrow-Hoff y algunas arquitecturas simples de las redes neuronales (es decir, el auto asociador lineal, el heteroasociador y el error de redes por propagación regresiva). Se muestra también que las redes neuronales son equivalente a versiones iterativas de la estadística estándar y modelos de optimización tales como el análisis de regresión multiple y el análisis por componentes principales.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>El álgebra lineal es usada particularmente para analizar la clase de redes neuronales denominadas «asociadores». Estos modelos de aprendizaje cuantitativo asocian una «entrada» y una «salida» mediante patrones adaptativos que hacen uso de algoritmos de aprendizaje. Cuando el conjunto de patrones de entrada es diferente del conjunto de salida, los modelos se denominan <em>heteroasociadores</em>. Cuando los patrones de entrada y los de salida son iguales, el modelo se denomina <em>autoasociador</em>. Los asociadores consisten de capas de unidades elementales denominadas <em>neuronas</em>. La información fluye a través de todas las capas. Algunas arquitecturas puede incluir capas intermedias (capas ocultas). Típicamente las neuronas de una capa están conectadas con las neuronas de otra capa. El objetivo será entender como algunas operaciones del álgebra lineal describe las transformaciones de la información a través de cada una de las capas.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Como es usual, los vectores serán representados por letras minúsculas (p. ej., $x$), las matrices por letras mayúsculas (p. ej.., $X$). Además se supone que las siguientes nociones son conocidas: La operación de transposición (p. ej., $x^\top$), la norma de un vector (p. ej.,$||x||$), el producto escalar ( p. ej., $x^{\top}w$) y el producto de dos matrices ( p. ej., $AB)$. También se usará el producto de Hadamard (p. ej., $A\odot B$).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Proyecci&#243;n-de-un-vector">Proyecci&#243;n de un vector<a class="anchor-link" href="#Proyecci&#243;n-de-un-vector">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Coseno-entre-dos-vectores">Coseno entre dos vectores<a class="anchor-link" href="#Coseno-entre-dos-vectores">&#182;</a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>El <em>coseno</em> entre dos vectores $x$ y $y$ es el coseno del ángulo formado por el origen del espacio y los puntos definidos por las coordenadas de los vectores. Por lo tanto,</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{equation}\nonumber
\cos(x, y) = \frac{x^\top y}{||x||||y||}.
\end{equation}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>El coseno indica la <em>similaridad</em> entre los vectores. Cuando dos vectores tienen la misma dirección (p. ej. $v=\lambda w, \lambda &gt;0$), su coseno es igual a uno; si tienen dirección opuesta (p. ej. $v=\lambda w, \lambda &lt;0$), su coseno es igual a menos uno; y cuando ellos son ortogonales (p. ej. $v^{\top}w=0$), su coseno es igual a cero.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Distancia-entre-vectores">Distancia entre vectores<a class="anchor-link" href="#Distancia-entre-vectores">&#182;</a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Entre una gran familia de distancias entre vectores, la más popular es la distancia euclidiana. Ésta está relacionada con el coseno entre vectores y se define como</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{equation}\nonumber
d_{2}(x,y)=\sqrt{(x-y)^\top(x-y)}=||(x-y)||
\end{equation}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Proyecci&#243;n-de-un-vector-sobre-otro-vector">Proyecci&#243;n de un vector sobre otro vector<a class="anchor-link" href="#Proyecci&#243;n-de-un-vector-sobre-otro-vector">&#182;</a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>La proyección ortogonal de un vector $x$ sobre un vector $w$ se define como:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{equation}\nonumber
\operatorname{proy}_{\langle w\rangle} x = \frac{x^{\top} w}{w^{\top}w}w.
\end{equation}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>La norma de $\operatorname*{proy}_{\langle w\rangle} x$ es su distancia al origen del espacio. Esto es igual a:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{equation}\nonumber
||\operatorname{proy}_{\langle w\rangle} x||=\frac{|x^{\top}w|}{||w||}=|\cos(x,y)|||x||.
\end{equation}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Las-reglas-de-aprendizaje-de-Hebbian-y-Widrow-Hoff">Las reglas de aprendizaje de Hebbian y Widrow-Hoff<a class="anchor-link" href="#Las-reglas-de-aprendizaje-de-Hebbian-y-Widrow-Hoff">&#182;</a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Una red neuronal consiste de células conectadas a otras células vía conexiones de peso denominadas <em>sinapsis</em>. Considere una red neuronal de $m$ entradas dada por una capa de células y solo una célula de salida. La información es transmitida vía la sinapsis, del conjunto de entrada de las células externas a las células de salida con la respuesta correspondiente al estado de activación. Si el patrón de entrada y el conjunto de pesos sinápticos son dados por un vector $m$ - dimensional denotado por $x$, y $w$, la activación de la célula de salida es dada por</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{equation}\nonumber
a = x^{\top}w.
\end{equation}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Así la activación es proporcional a la norma de la proyección del vector de entradas sobre el vector de pesos. La respuesta o salida de la célula es denotada por $r$. Para una célula lineal, esta es proporcional a la activación (por conveniencia, se asume que la constante de proporcionalidad es igual a uno). Los heteroasociadores lineales y los autoasociadores son construidos con células lineales. En general, la salida de una célula es una función (no necesariamente continua), denominada la función de transferencia, y su activación es:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{equation}\nonumber
\label{eqn:función}
r = f(a).
\end{equation}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Por ejemplo, en redes de retropropagación, la función de transferencia (no lineal) suele ser la función logística</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{equation}\nonumber
r = f(a) = \operatorname{logit}(w^{\top}x) = \frac{1}{1 + \exp(-w^{\top}x)}.
\end{equation}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A menudo,una red neuronal está diseñada para asociar, a una entrada dada, una respuesta específica llamada objetivo, denotada como $t$. El aprendizaje es equivalente a definir una regla que especifique cómo agregar una pequeña cantidad a cada peso sináptico en cada iteración del algoritmo. El algoritmo acerca la salida de la red al objetivo.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Las reglas de aprendizaje vienen en dos sabores principales: <em>supervisadas</em> (p. ej. Widrow-Hoff) que tienen en cuenta el error o la distancia entre la respuesta de la neurona y el objetivo,  y <em>sin supervisión</em> (p ej. Hebbian) que no requieren tal «retroalimentación». La regla de aprendizaje hebbiana modifica el vector de peso en la iteración $k + 1$ como</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{equation}\nonumber
w_{k+1} = w_{k} + \eta t x,
\end{equation}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>donde $\eta$ es una pequeña constante positiva llamada <em>constante de aprendizaje</em>. Entonces, en cada iteración de la relgla aprendizaje hebbiana se mueve el vector de peso en la dirección del vector de entrada en una cantidad proporcional al objetivo.</p>
<p>La regla de aprendizaje de Widrow-Hoff utiliza el error y la derivada de la función de transferencia $f$ para calcular la corrección como:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{equation}\nonumber
\label{eqn:corrección}
w_{k+1} = w_{k} + \eta (t-r_{k})f'(a_k)x.
\end{equation}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Entonces, una iteración de aprendizaje de Widrow-Hoff mueve el vector de peso en la dirección del vector de entrada en una cantidad proporcional al error.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Para redes con varias celulas (p. ej. $n$) en la capa de salida, el patrón de activación, salida y objetivo se convierten en vectores $n$ - dimensionales (denotados $a$, $r$ y $t$, respectivamente), y los pesos sinápticos se almacenan en una matriz $W$ de dimensión  $m \times n$. Las ecuaciones de aprendizaje se reescriben de la siguiente forma:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{equation}\nonumber
W_{k+1} = W_{k}+\eta x t^{\top}\hbox{(Hebbian)}
\end{equation}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{equation}\nonumber
W_{k+1} = W_{k}+\eta(f'(a_{k})\odot x)(t- r_{k})^{\top} \hbox{ (Widrow-Hoff)},
\end{equation}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>en donde la derivada $f'$ aplica sobre $a$ por cada componente, es decir, $f'(a)=(f'(a_{1}),\dots,f'(a_{n}))$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>En general, se deben aprender varias ($l$) asociaciones de entrada / destino. Luego, el conjunto de patrones de entrada se almacena en una matriz $m \times l$ denotada como $X$, los patrones de activación y objetivo respectivamente se almacenan en matrices de dimensión $n \times l$ indicadas como $A$ y $T$, respectivamente. Las iteraciones de activación y aprendizaje se pueden calcular para todos los patrones a la vez (esto se llama aprendizaje por lotes). La matriz de salida se calcula como:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{equation}\nonumber
r = f(A) = f(WX^{T}),    
\end{equation}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>en donde   $f$ también aplica sobre cada componente de $WX^{\top}$, es decir $f(WX^{\top})=[f([WX^{\top}]_{ij})]$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Las ecuaciones de aprendizaje se convierten</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a id='1'></a>
\begin{equation}\tag{1}
W_{k+1} = W_{k} + \eta X T^{\top} \hbox{ (Hebbian)},
\end{equation}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a id='2'></a>
\begin{equation}\tag{2}
W_{k+1} = W_{k} + \eta (f'(A_{k}) \odot X)(T- R_{k})^{\top} \hbox{ (Widrow-Hoff).}
\end{equation}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Valores-propios,-vectores-propios-y-la-descomposici&#243;n-en-valores-singulares">Valores propios, vectores propios y la descomposici&#243;n en valores singulares<a class="anchor-link" href="#Valores-propios,-vectores-propios-y-la-descomposici&#243;n-en-valores-singulares">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Los vectores propios de una matriz cuadrada $W$ dada (resultante de su <em>descomposición propia</em>) son vectores invariantes bajo multiplicación por $W$. La descomposición propia se define mejor para una subclase de matrices llamadas matrices <em>semi-definidas</em> positivas. Una matriz $X$ es positiva semi-definida si existe otra matriz $Y$ tal que $X = YY^{\top}$. Este es el caso de la mayoría de las matrices utilizadas en redes neuronales, por lo que se considera solo este caso aquí.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Formalmente, un vector (distinto de cero) $u$ es un vector propio de una matriz cuadrada $W$ si</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{equation}\nonumber
Wu = \lambda u.
\end{equation}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>El escalar $\lambda$ es el valor propio asociado con $u$. Entonces $u$ es un vector propio de $W$ si su dirección es invariante bajo la multiplicación por $W$ (solo su longitud cambia si $\lambda \neq 1$). En general, hay varios vectores propios para una matriz dada (como máximo, la dimensión de $W$).  En general, se ordenan por orden decreciente de su valor propio. Entonces, el primer vector propio, $u_{1}$ tiene el mayor valor propio $\lambda_{1}$. El número de vectores propios con un valor propio distinto de cero es el rango de la matriz.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Los valores propios de las matrices semidefinidas positivas son siempre positivos o cero (una matriz con valores propios estrictamente positivos, es definida positiva). Además, cualquier par de vectores propios $u_i$, $u_j$, con valores propios diferentes, son ortogonales, es decir:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{equation}\nonumber
u_i^{\top} u_{j} = 0 \,\, \forall\,\, i \neq j.
\end{equation}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Además, el conjunto de vectores propios de una matriz constituye una base ortogonal para los espacios fila y  columna. Esto se expresa definiendo dos matrices, la matriz de vectores propios $U$, y la matriz diagonal de los valores propios $\Lambda$. Así la descomposición propia de $W$ (con rango $n$) es:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{equation}\nonumber
W = U \Lambda U^{\top}.
\end{equation}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>La descomposición de valores singulares (SVD) generaliza la descomposición propia en matrices rectangulares. Si $X$ es una matriz $m \times l$, su SVD se define como:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{equation}\nonumber
X = U \Delta V^{\top}
\end{equation}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>con $U U^{\top} = V^{\top} V = I$ y $\Delta$ una matriz diagonal ($I$ siendo la matriz identidad).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Los elementos diagonales de $\Delta$ son números reales positivos llamados valores singulares de $X$. Las matrices $U$ y $V$ son las matrices izquierda y derecha de vectores singulares (que también son vectores propios, ver más abajo).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>El SVD está estrechamente relacionado con la descomposición propia porque $U$, $V$ y $\Delta$ pueden obtenerse a partir de la descomposición propia de las matrices $X^{\top} X$ y $X X^{\top}$ como</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{equation}\nonumber
X^{\top} X = U \Lambda U^{\top},\,\, X X^{\top} = V \Lambda V^{\top},\hbox{ y } \Delta = \Lambda^{1/2}.
\end{equation}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Tenga en cuenta que $X^{\top} X$ y $X X^{\top}$ tienen los mismos valores propios.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Las descomposiciones de valores propios y singulares se utilizan en la mayoría de los campos de las matemáticas aplicadas, incluidas las estadística, el procesamiento de imágenes, la mecánica y los sistemas dinámicos. Para las redes neuronales, son esenciales para estudiar la dinámica de los autoasociadores y heteroasociadores lineales.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Procesos-iterativos">Procesos iterativos<a class="anchor-link" href="#Procesos-iterativos">&#182;</a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Un heteroasociador lineal que usa la regla de Widrow-Hoff, el aprendizaje modifica solo los valores propios de la matriz de peso. Específicamente, si los patrones a aprender se almacenan en una matriz $X$ de orden $m \times l$, con una descomposición de valores singulares como $X = U \Delta V^{\top}$, entonces la <a href="#2">Ecuación 2</a> de la regla aprendizaje de Widrow-Hoff se convierte en</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a id='1'></a>
\begin{equation}\tag{3}
W_{k+1} = W_{k} + \eta X(T - R_{k})^{\top} = U\Delta^{-1}[I - (I - \eta\Delta^2)^{n+1}] V^{\top} T^{\top},
\end{equation}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>porque para un heteroasociador lineal $A_{k}=R_{k}$ y $f'(R_{k}) = I$. (ver Abdi, 1994, p.54 ff.).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>La matriz de peso de Widrow-Hoff corresponde a la primera iteración del algoritmo, es decir,</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{equation}\nonumber
W_{1} = U\Delta^{-1}[I - (I - \eta \Delta^2)] V^{\top} T^{\top} = \eta U \Delta V^{\top} T^{\top} = \eta X T^{\top}.
\end{equation}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>La ecuación <a href="#3">Ecuación 3</a> caracteriza los valores de $\eta$ que permiten que el proceso iterativo converja. Denotando por $\delta_{max}$ el mayor valor singular de $X$, si $\eta$ es tal que:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a id='4'></a>
\begin{equation}\tag{4}
0 &lt; \eta &lt; 2 \delta^{-2}_{max}
\end{equation}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>entonces se puede demostrar que (ver Abdi, 1994)</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{equation}\nonumber
\lim_{n \to \infty}(I - \eta \Delta^{2})^{n} = 0
\end{equation}<p>y por lo tanto</p>
\begin{equation}\nonumber
\lim_{n \to \infty} W_{n} = U \Delta^{-1} V^{\top} T^{\top} = X^{+} T.
\end{equation}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>La matriz $X^{+} = U \Delta^{-1} V^{\top} $ es el pseudoinverso de $X$. Da una solución óptima de mínimos cuadrados para la asociación entre la entrada y el objetivo. Por lo tanto, el heteroasociador lineal es equivalente a la regresión múltiple lineal. Si $\eta$ está fuera del intervalo definido por la <a href="#4">Ecuación 4</a>, tanto los valores singulares como los elementos de la matriz de peso crecerán en cada iteración. En la práctica, debido a que las redes neuronales son simuladas por computadoras digitales, la matriz de peso eventualmente alcanzará los límites de la precisión de la máquina.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Cuando los vectores objetivo son los mismos que los vectores de entrada (es decir, cuando cada entrada está asociada a sí misma), el heteroasociador lineal se convierte en un autoasociador lineal. El enfoque anterior muestra que, ahora, la matriz Hebbiana de peso  es la matriz de productos cruzados:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{equation}\nonumber
W_{1} = X X^{\top} = U \Lambda U^{\top}.
\end{equation}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Con el aprendizaje de Widrow-Hoff, cuando se alcanza la convergencia, todos los valores propios distintos de cero de la matriz de peso son iguales a 1. La matriz de peso se dice que es esférica; esto es igual a:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{equation}\nonumber
W_{\infty}  = U U^{\top}.
\end{equation}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Debido a que la técnica estadística del análisis de componentes principales (PCA) calcula la descomposición propia de una matriz de productos cruzados similar a $W$, el autoasociador lineal se considera como la red neuronal equivalente de PCA.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Optimizaci&#243;n,-Derivadas-y-Matrices">Optimizaci&#243;n, Derivadas y Matrices<a class="anchor-link" href="#Optimizaci&#243;n,-Derivadas-y-Matrices">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Las redes neuronales se utilizan a menudo para optimizar una función de los pesos sinápticos. La <em>diferenciación</em> de una función es el concepto principal para explorar problemas de <em>optimización</em> y, para redes neuronales, implica la diferenciación de vectores o funciones matriciales. En este contexto, debemos considerar la función de transferencia como una función del vector de peso. Esto es:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{equation}\nonumber
r = f(w).
\end{equation}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>La derivada de $f(w)$ con respecto al vector $w$ de $m$ - dimensional se denota por $\nabla f (w)$. También se llama el gradiente de $f$, es decir,</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{equation}\nonumber
\nabla f(w) = \frac{\partial f}{\partial w} = \left[\frac{\partial f}{\partial w_{1}},..., \frac{\partial f}{\partial w_{i}},..., \frac{\partial f}{\partial w_{I}} \right]^{\top}.
\end{equation}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Por ejemplo, la derivada de la salida de una neurona lineal es</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{equation}\nonumber
\frac{\partial f}{\partial w} = \left[\frac{\partial w^{\top} x}{\partial w_{1}},\dots, \frac{\partial w^{\top} x}{\partial w_{m}} \right]^{\top} = [x_{1},\dots, x_{m}]^{\top} = x.
\end{equation}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Cuando una función es dos veces diferenciable, las derivadas de segundo orden se almacenan en una matriz llamada matriz Hessiana de la función. A menudo se denota por $\nabla^{2}(f)$ (recuerde que $\nabla^{2}(f) = [\nabla\nabla^{\top}](f)$)  y se define formalmente como</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{equation}\nonumber
\nabla^{2}(f) = \left[\begin{array}{cccc}
\frac{\partial^{2}_{f}}{\partial w^{2}_{1}} &amp; \frac{\partial^{2}_{f}}{\partial w_{1} w_{2}} &amp; \cdots &amp; \frac{\partial^{2}_{f}}{\partial w_{1} w_{m}} \\ 
\frac{\partial^{2}_{f}}{\partial w_{2} w_{1}} &amp; \frac{\partial^{2}_{f}}{\partial w^{2}_{2}} &amp; \cdots &amp; \frac{\partial^{2}_{f}}{\partial w_{2} w_{m}} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\frac{\partial^{2}_{f}}{\partial w_{I}w_{1}} &amp; \frac{\partial^{2}_{f}}{\partial w_{I}w_{2}} &amp; \cdots &amp; \frac{\partial^{2}_{f}}{\partial w^{2}_{m}}.
\end{array} \right].
\end{equation}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Condiciones-para-m&#237;nimo">Condiciones para m&#237;nimo<a class="anchor-link" href="#Condiciones-para-m&#237;nimo">&#182;</a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Un problema estándar es mostrar que una regla de aprendizaje dada encuentra una solución óptima en el sentido de que una función del vector de peso (o matriz) llamada <em>función de error</em> alcanza su valor mínimo cuando el aprendizaje ha convergido. A menudo, la función de error se define como la suma del error al cuadrado sobre todos los patrones.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Cuando se puede evaluar el gradiente de la función de error, una condición necesaria para la optimización (es decir, mínimo o máximo) es encontrar un vector de peso $w^{*}$ tal que</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{equation}\nonumber
\nabla f (w^{*}) = 0.
\end{equation}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Esta condición también es suficiente siempre que $\nabla^{2}(f)$ sea definida positiva (cf. Haykin, 1999).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Expansi&#243;n-de-Taylor">Expansi&#243;n de Taylor<a class="anchor-link" href="#Expansi&#243;n-de-Taylor">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>La expansión de Taylor es la técnica estándar utilizada para obtener una aproximación lineal o cuadrática de una función de una variable. Recuerde que la expansión de Taylor de una función continua $f(x)$ es</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{equation}\nonumber
f(x) = \sum_{n=0}^{\infty}(x-a)^{n}\frac{f^{(n)}(a)}{n!} = f(a) +(x-a)\frac{f'(a)}{1!} + (x-a)^{2} \frac{f''(a)}{2!} + \mathcal{R}_{2},
\end{equation}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>en donde $\mathcal{R}_{2}$ representa todos los términos de orden superior a 2, y $a$ es un valor «conveniente» para evaluar $f$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Esta técnica puede extenderse a funciones de matrices y vectores. Implica la noción de gradiente y de Hessiano. Ahora para una función vectorial $f(x)$ se expresa como:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{equation}\nonumber
f(x) = f(a) + f(x - a)^{\top}\nabla f(a) + f(x - a)^{\top} \nabla^{2}f(a)f(x - a) + \mathcal{R}_{2}.
\end{equation}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Minimizaci&#243;n-iterativa">Minimizaci&#243;n iterativa<a class="anchor-link" href="#Minimizaci&#243;n-iterativa">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Se puede demostrar que una regla de aprendizaje converge a un valor óptimo si disminuye el valor de la función de error en cada iteración. Cuando se puede evaluar el gradiente de la función de error, la técnica de <em>gradiente</em> (o <em>descenso más pronunciado</em>) ajusta el vector de peso moviéndolo en la dirección opuesta al gradiente de la función de error. Formalmente, la corrección para la iteración $(k + 1)$ es</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{equation}\nonumber
w_{k+1} = w_{k} + \nabla = w_{k} - \eta \nabla f(w_k)
\end{equation}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Como ejemplo, demostremos que para un heteroasociador lineal, la regla de aprendizaje de Widrow-Hoff minimiza iterativamente el error al cuadrado entre el objetivo y la salida. La función de error es</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{equation}\nonumber
e^{2} = (t-r)^{2} = t^{2} + r^{2} - 2tr = t^{2} + x^{\top}w w^{\top} -2tw^{\top}x.
\end{equation}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>El gradiente de la función de error es:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{equation}\nonumber
\frac{\partial e}{\partial w} = 2(w^{\top} x)x - 2t x = -2(t - w^{\top} x) x.
\end{equation}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>El vector de peso se corrige moviéndose en la dirección opuesta del gradiente.  Esto proporciona la siguiente corrección para la iteración $k + 1$:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{equation}\nonumber
w_{k+1} =  w_{k} - \eta \frac{\partial e}{\partial w} = w_{k} + \eta (t-w^{\top}_{k}x)x = w_{k} + \eta(t-o_{k})x.
\end{equation}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Esto da la regla de aprendizaje de Widrow-Hoff en su expresión más simple.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>El método de gradiente funciona porque el gradiente de $w_{n}$ es una aproximación de Taylor de primer orden del gradiente del vector de peso óptimo $w^{*}$. Es una técnica favorita en las redes neuronales porque el error de propagación posterior popular es una técnica de gradiente.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>El método de Newton es una aproximación de Taylor de segundo orden, utiliza el inverso del hessiano de $w$ (suponiendo que exista). Proporciona una mejor aproximación numérica pero requiere más cómputo. Aquí la corrección para la iteración $k + 1$ es</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{equation}\nonumber
w_{k+1} = w_{k} - [\nabla^{2}f(w_{k})]^{-1} \nabla_{f(w_{k})}.
\end{equation}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Referencias">Referencias<a class="anchor-link" href="#Referencias">&#182;</a></h2><ul>
<li>Hervé Abdi. 2001. Linear Algebra for Neural Networks.</li>
<li>Hervé Abdi et al. 1999. Neural networks. Thousand Oak.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Contacto">Contacto<a class="anchor-link" href="#Contacto">&#182;</a></h2><ul>
<li>Participa de la canal de Nerve a través de <a src='https://discord.gg/edPmghPq8K'>Discord</a>.</li>
<li>Se quieres conocer más acerca de este tema me puedes contactar a través de <a src='https://www.classgap.com/me/alejandro-sanchez-yali'>Classgap</a>.</li>
</ul>

</div>
</div>
</div>
 

