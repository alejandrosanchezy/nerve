<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>El objetivo del algoritmo del gradiente descendente (AGD) es la minimización de una función diferenciable convexa<a href="#10"><sup>1</a>, $f:\Omega\subset\mathbb{R}^n \to \mathbb{R}$, es decir, permite encontrar un $w$ en $\Omega$ tal que $f(w)$ es un mínimo, en otras palabras, se utiliza para determinar los elementos del siguiente conjunto:</p>
<p><a id='1'></a>
\begin{equation}\tag{1}
w \in \operatorname*{argmin\,\,}_{ w\in \Omega} f(w) 
\end{equation}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Este algoritmo es una serie de iteracciones que se hacen de acuerdo con la siguiente regla de actualización:</p>
<p><a id='2'></a>
\begin{equation}\tag{2}
    w_{t+1} = w_{t} -\eta_{t} \nabla f(w_{t}),
\end{equation}</p>
<p>que usualmente se inicializa en cero y cada iteración, como se puede observar, se hace en la dirección negativa del gradiente. Recuerde que el gradiente se define como el vector:</p>
\begin{equation}\nonumber
\nabla f(w)=\left(\frac{\partial f}{\partial x_{1}}(w),\dots, \frac{\partial f}{\partial x_{n}}(w)\right).
\end{equation}<p>Además, también se puede observar en este algoritmo la presencia de la variable $\eta_{t} &gt; 0$,  que se denominada usualmente como <em>la tasa de aprendizaje en la iteración $t$</em> y que como se verá más adelante ésta es definida como un parámetro externo del algoritmo.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="&#191;C&#243;mo-funciona-este-algoritmo?">&#191;C&#243;mo funciona este algoritmo?<a class="anchor-link" href="#&#191;C&#243;mo-funciona-este-algoritmo?">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Este algoritmo se define a partir de las dos características esenciales que tiene el gradiente, las cuales se mencionan a continuación:</p>
<ol>
<li>El gradiente es perpendicular a las curvas de nivel de $f$, de manera que para cualquier dirección $v\in \mathbb{R}^{n}$ ortogonal a $\nabla f(p)$, es una dirección de cambio nulo. Esto se observa facilmente al parametrizar la curva $S_{k}=\{p\in \Omega: f(p)=k\}$  mediante una función $\alpha:I\subset \mathbb{R}\to S_{k}$ tal que $\alpha(0)=p$, pues al calcular el producto punto de $\nabla f(p)$ con la velocidad de $\alpha$ en $p$ se obtiene que la tasa de cambio es:<br><br>\begin{equation}\nonumber|df_{p}(\alpha'(0))|=|\nabla f(p)\cdot \alpha' (0)| = 0, \end{equation}<br> es decir, la tasa de cambio en la dirección de $\alpha'(0)$ es cero.</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a id='1'></a></p>
<div align="center">
<img src="figures/gd.svg">
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ol>
<li>El gradiente indica la dirección ascendente de la tasa de máximo cambio de $f$ en el punto $p$. La tasa máxima se calcula como $||\nabla f(p)||$. La razón de esto, se aprecia cuando se considera un vector $v\in \mathbb{R}$ tal que $||v||=1$, de manera que para este vector la tasa de cambio es:<br><br>\begin{equation}\nonumber |df_{p}(v)|=||\nabla f(p)||||v|||cos\theta|\leq ||\nabla f(p)|| \end{equation}<br> Dicha magnitud es máxima cuando $\theta = 2n\pi$ con $n\in \mathbb{Z}$, es decir, para que $|df_{p}(v)|$ sea máxima, los vectores $\nabla f(p)$ y $v$ deben ser paralelos, de esta manera,  la función $f$ crece más rápidamente en la dirección del vector $\nabla f(p)$ y decrece más rápidamente en la dirección de $-\nabla f(p)$, en efecto, si $v=\frac{\nabla f(p)}{||\nabla f(p)||}$, entonces $df_{p}(v)=||\nabla f(p)||$.</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>La iteración definida en la <a href="#2">Ecuación 2</a> permite construir una sucesión de puntos $\{w_t\}_{t\in [m]_{\mathbb{N}_0}}$ de tal manera que $f(w_{t+1})&lt;f(w_{t})$. Este hecho se puede evidenciar mediante el polinomio de Taylor, en efecto si se considera un punto inicial $w_{0}$, entonces para el primer termino de la expansión de Taylor alrededor de $w_{0}$ se tiene:</p>
\begin{equation}\nonumber
f(w_{1})-f(w_{0})\approx \langle w_{1}-w_{0}, \nabla f(w_{0})\rangle=-\eta ||\nabla f(w_{o})||^2
\end{equation}<p>Por consiguiente:</p>
\begin{equation}\nonumber
f(w_{1})-f(w_{0}))=-\eta ||\nabla f(w_{o})||^2 + o(\eta)
\end{equation}<p>de tal manera que para un $\eta$ adecuado se puede garantizar que $f(w_{1})&lt;f(w_{0})$. El razanomiento se puede repetir para $w_{t+1}$ y $w_{t}$, de tal manera que $w_{t+1}$ es una mejora de $w_{t}$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>De acuerdo a las consideraciones anteriores, el algoritmo del gradiente descendente se define de la siguiente manera:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>   
    <u><strong>Algoritmo del gradiente descendente (AGD)</strong></u>
<br>
Input: $w_0$, $m$, $\eta$, $\nabla f(w)$
<br>&nbsp; &nbsp;&nbsp;for $k=0$ to $m$ do:
<br>&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;$w\leftarrow w - \eta \nabla f(w)$
<br>&nbsp; &nbsp;&nbsp;end    
<br>return: $w$

</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>donde $w_0$ es la condición inicial del algoritmo, $m$ es el número máximo de iteraciones, $\eta$ es la tasa de aprendizaje y $\nabla f(w)$ es la función gradiente de $f$. Observe que la regla de actualización se definió apartir de la <a href="#2">Ecuación 2</a>. Es importante notar que finalmente $w_{t+1}$ es tal que:</p>
\begin{equation}\nonumber
w_{t+1}\in \operatorname*{argmin\,\,}_{ w\in \Omega} \frac{1}{2\eta_{t}}||w-w_{t}+\eta_{t}\nabla f(w_{t})||.
\end{equation}<p>Es fácil comprobar que el problema de optimización anterior se puede reescribir como:</p>
\begin{equation}
\operatorname*{argmin\,\,}_{ w\in \Omega} \frac{1}{2\eta_{t}}||w-w_{t}+\eta_{t}\nabla f(w_{t})||=\operatorname*{argmin\,\,}_{ w\in \Omega} \left(f(w_{t}) + \langle \nabla f(w_t), w-w_t \rangle + \frac{1}{2\eta_{t}}||w-w_{t}||\right)
\end{equation}<p>Por lo tanto, el $w_{t+1}$ es obtenido para minimizar la linealización de la función $f$ alrededor del punto $w_{t}$, manteniendolo lo suficientemente aproximado a este el punto $w_{t}$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="&#191;Cu&#225;ndo-parar-las-iteraciones?">&#191;Cu&#225;ndo parar las iteraciones?<a class="anchor-link" href="#&#191;Cu&#225;ndo-parar-las-iteraciones?">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Es importante aclarar, que a pesar de que el algoritmo ejecute el máximo de iteraciones, el resultado arrojado no necesariamente es una buena aproximación a el elemento minimizador de la función $f$. Por lo tanto es necesario definir un criterio que permita decidir si el resultado obteniendo es adecuado o no.</p>
<p>Como primer criterio de parada del algoritmo que se le puede ocurrir al lector es detener las iteraciones cuando $||\nabla f(x_{t})||=0$, pero esto no es práctico debido a diferentes factores que influyen, como el comportamiento de la coma flotante y la elección adecuada de la tasa de aprendizaje. Usualmente en la práctica se suele definir un parametro de tolerancia $\epsilon&gt;0$ que junto con alguno de los siguientes criterios se usan para detener las iteraciones:</p>
<ul>
<li><strong>Condición sobre el gradiente:</strong> <br><br>\begin{equation}\nonumber ||\nabla f(x_{t})||&lt;\epsilon\end{equation}<br></li>
<li><strong>Condición sobre las diferencias sucesivas relativas de la función objetivo:</strong> <br><br>\begin{equation}\nonumber \frac{|f(x_{t+1})-f(x_{t})|}{|f(x_{t})|}&lt;\epsilon,\end{equation}<br> si el denominador es muy pequeño, es conveniente remplazarlo por $\max\{1, |f(x)|\}$.</li>
<li><strong>Condición sobre las diferencias sucesivas relativas de la variable independiente:</strong> <br><br>\begin{equation}\nonumber \frac{||x_{t+1}-x_{t}||}{||x_{t}||}&lt;\epsilon,\end{equation}<br> si el denominador es muy pequeño, es conveniente remplazarlo por $\max\{1, ||x_{t}||\}$.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Algunas-consideraciones-sobre-las-tasas-de-aprendizaje.">Algunas consideraciones sobre las tasas de aprendizaje.<a class="anchor-link" href="#Algunas-consideraciones-sobre-las-tasas-de-aprendizaje.">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><p>La principal desventaja del AGD se encuentra en el ajuste adecuado de la tasa de aprendizaje $\eta$. Si $\eta$ toma un valor muy pequeño, es necesario un gran número de iteraciones para que el proceso converga; si por otro lado $\eta$ es muy grande, entonces puede ocurrir que el proceso no converga.</p>
</li>
<li><p>La tasa de aprendizaje $\eta$ es determinada por la minimización exacta de:<br><br>\begin{equation}\nonumber\eta_{t} \in \operatorname*{argmin\,\,}_{\eta&gt;0}f(x_{t}-\eta\nabla f(x_{t})). \end{equation}<br> Esto se usa principalmente para problemas de caracter cuadrático y en donde el cálculo de $\eta$ es económico pero una evaluación de gradiente costosa; de lo contrario, no vale la pena el esfuerzo de resolver este subproblema exactamente.</p>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Ejemplo:-Gradiente-descente-para-una-forma-cuadr&#225;tica">Ejemplo: Gradiente descente para una forma cuadr&#225;tica<a class="anchor-link" href="#Ejemplo:-Gradiente-descente-para-una-forma-cuadr&#225;tica">&#182;</a></h2><p>Asuma que $Q$ es simétrica y definida positiva ($x^{\top}Q x&gt;0$ para cualquier $x\neq 0$). Considere la forma cuadrática:</p>
\begin{equation}\nonumber
f(x)=\frac{1}{2}x^{\top}Qx - b^{\top}x
\end{equation}<p>el lector puede comprobar que su gradiente es:</p>
\begin{equation}\nonumber
\nabla f(x)=Qx-b.
\end{equation}<p>Así la secuencia de $\{x_{t}\}_{t\in [m]}$ que inicia en cualquier $x_{0}$ viene dada por:</p>
\begin{equation}\nonumber
x_{t+1}=x_{t}-\eta_{k}(Qx-b)
\end{equation}<p>con $g_{t}:=\nabla f(x_t)$ se  define:</p>
\begin{equation}\nonumber
\eta_{t}=\frac{g_{t}^{\top}g_{t}}{g_{t}^{\top}Qg_{t}}.
\end{equation}<p>El lector puede validar que con el valor de $\eta_{t}$ definido anteriormente se tiene:</p>
\begin{equation}\nonumber
\eta_{t}\in \operatorname*{argmin\,\,}_{\eta&gt;0}f(x_{t}-\eta\nabla f(x_{t})).
\end{equation}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="&#191;C&#243;mo-implementarlo-en-python?">&#191;C&#243;mo implementarlo en python?<a class="anchor-link" href="#&#191;C&#243;mo-implementarlo-en-python?">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Para las consideraciones del ejemplo anterior, es fácil definir una clase en Python para todas las formas cuadraticas, junto con tres métodos principales que permiten evaluar la forma cuadrática, calcular $\eta$  y  el gradiente en un punto $x$. Esto sería algo así:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">class</span> <span class="nc">QuadraticForm</span><span class="p">:</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Inputs:</span>
<span class="sd">            Q: Positive definite symmetric matrix.</span>
<span class="sd">            b: Rn vector.        </span>
<span class="sd">        &quot;&quot;&quot;</span>        
        <span class="bp">self</span><span class="o">.</span><span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span>        
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">b</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Method to evaluate the quadratic form.</span>
<span class="sd">        Inputs:</span>
<span class="sd">            x: Rn vector.</span>
<span class="sd">        Ouput:</span>
<span class="sd">            Value of the quadratic form in x. </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q</span>
        <span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span>    
        <span class="k">return</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Q</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">-</span> <span class="n">b</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">eta</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>  
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Method to evaluate eta.</span>
<span class="sd">        Inputs:</span>
<span class="sd">            x: Rn vector.</span>
<span class="sd">        Output:</span>
<span class="sd">            Value of eta in x. </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">gradient_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  
        <span class="n">numerator</span> <span class="o">=</span> <span class="n">gradient_x</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">gradient_x</span><span class="p">)</span>
        <span class="n">denominator</span> <span class="o">=</span> <span class="n">gradient_x</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Q</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">gradient_x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">numerator</span> <span class="o">/</span> <span class="n">denominator</span>
    
    <span class="k">def</span> <span class="nf">gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Method to evaluate the gradient.</span>
<span class="sd">        Inputs:</span>
<span class="sd">            x: Rn vector.</span>
<span class="sd">        Output:</span>
<span class="sd">            Value of the gradient in x.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Q</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">b</span>    
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>El AGD se implementaría de la siguiente forma:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[8]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">initial_x</span><span class="p">,</span> <span class="n">eta</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">function</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gradient descent Algorithm.</span>
<span class="sd">    Inputs:</span>
<span class="sd">        initial_x: Rn vector.</span>
<span class="sd">        eta: learning rate.</span>
<span class="sd">        epsilon: precision.</span>
<span class="sd">        function: Quadractic Form.</span>
<span class="sd">    Output:</span>
<span class="sd">        Point where the function reaches the minimum.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">initial_x</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        
        <span class="n">gradient_x</span> <span class="o">=</span> <span class="n">function</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> 

        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">gradient_x</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;stop: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">))</span>
            <span class="k">break</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">eta</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">)):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">gradient_x</span>
        <span class="k">else</span><span class="p">:</span>  
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">eta</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">gradient_x</span>
            
        <span class="n">k</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">return</span> <span class="n">x</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Para ejemplicar el funcionamiento del código anterior, se considera la matriz simétrica y positiva definida:</p>
\begin{equation}\nonumber
Q = \left(\begin{array}{cc} 1 &amp; 0.5 \\ 0.5 &amp; 3 \end{array}\right)
\end{equation}<p>y vector $b$ dado por:</p>
\begin{equation}\nonumber
b = \left(\begin{array}{c} 3 \\ 0.5 \end{array}\right)
\end{equation}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>De esta manera la forma cuadrática en Python y usando código anterior quedaría así:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[9]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[10]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">quadratic_form</span> <span class="o">=</span> <span class="n">QuadraticForm</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Para encontrar el $x$ minimizador de la función se tiene dos opciones, una es definir el parámetro $\eta$ manualmente, y la otra es usar el método <code>eta</code> de la clase <code>QuadracticForm</code>. A continuación se hará uso de los dos casos.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Para empezar se ejecuta el algoritmo en algún punto $x$:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[11]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">initial_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">105.5</span><span class="p">,</span> <span class="mf">105.8</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Luego ejecuta el AGD con $\eta = 0.0001$ y $\epsilon=0.0000001$:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[12]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">gradient_descent</span><span class="p">(</span><span class="n">initial_x</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.0000001</span><span class="p">,</span> <span class="n">function</span><span class="o">=</span><span class="n">quadratic_form</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>stop: 230300
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt output_prompt">Out[12]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>array([[ 3.18181829],
       [-0.36363639]])</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Como el lector podrá notar el algoritmo tardó 230300 iteraciones para obtener un candidato al mínimo con la precisión deseada. A continuación se ejecuta usando el método <code>eta</code> de la forma cuadrática:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[13]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">gradient_descent</span><span class="p">(</span><span class="n">initial_x</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="n">quadratic_form</span><span class="o">.</span><span class="n">eta</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.0000001</span><span class="p">,</span> <span class="n">function</span><span class="o">=</span><span class="n">quadratic_form</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>stop: 15
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt output_prompt">Out[13]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>array([[ 3.1818182 ],
       [-0.36363637]])</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>En esta ocasión el algoritmo tardó 15 iteraciones. Esto representa una mejora en tiempo de ejecución bastante considerable con respecto a el experimento anterior.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>¿Cómo se puede asegurar que este es el mínimo de la función? Para este caso en particular, el mínimo ocurre cuando $Qx = b$, por lo tanto es suficiente con resolver este sistema lineal. Usando los métodos de la librería numpy se puede resolver rápidamente así:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[14]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[14]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>array([[ 3.18181818],
       [-0.36363636]])</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Como se puede observar, el resultado es muy aproximado al valor que se obtuvo al ejecutar AGD, por lo que el algoritmo funciona bastante bien.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Conclusiones">Conclusiones<a class="anchor-link" href="#Conclusiones">&#182;</a></h2><ul>
<li>Se logró aprender que el AGD es un algoritmo iterativo empleado principalmente para resolver problemas de optimización.</li>
<li>La principal desventaja del AGD se encuentra en el ajuste adecuado de la tasa de aprendizaje $\eta$. Si $\eta$ toma un valor muy pequeño, es necesario un gran número de iteraciones para que el proceso converga; si por otro lado $\eta$ es muy grande, entonces puede ocurrir que el proceso no converga.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Notas">Notas<a class="anchor-link" href="#Notas">&#182;</a></h2><p><a id="10">[1]</a>. En el contexto de este artículo, se dira que una función $f:\Omega \subset \mathbb{R}^{m}\to \mathbb{R}$ es diferenciable si tiene derivada continua en $\Omega$ y es convexa, si para todo $w, z\in \Omega$ y $\alpha \in [0, 1]$ se cumple la siguiente condición:</p>
\begin{equation}\nonumber
f(\alpha w + (1-\alpha)z)\leq \alpha f(w) + (1-\alpha)f(z).
\end{equation}<p>Note que la condición es valida para todo $m\geq 1$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a id='1'></a></p>
<div align="center">
<img src="figures/convexfunction.svg">
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Referencias">Referencias<a class="anchor-link" href="#Referencias">&#182;</a></h2><ul>
<li>Kris Hauser. <a src='http://homes.sice.indiana.edu/classes/spring2012/csci/b553-hauserk/gradient_descent.pdf'>Algorithms for optimization and learning. University of Indiana</a>.</li>
<li><a src ='https://www.andrewng.org/'>Andrew Ng</a>. <a scr='http://cs229.stanford.edu/materials.html'>Machine learning course materials</a>. Technical report, University of Stanford.</li>
<li>Batard, Thomas and Bertalmío, Marcelo. <a scr='https://hal.archives-ouvertes.fr/hal-00782496/document'>Generealized gradient on vector bundle - Aplication to image denoising</a>. Department of Information and Communication Technologies, 2013.</li>
</ul>

</div>
</div>
</div>
 

